# -*- coding: utf-8 -*-
"""flower_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xn1oUsyjeOAa2rBNGhWyTN8hIUZBtJP1
"""

import os
from PIL import Image
import matplotlib.pyplot as plt
import random
import shutil
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # type: ignore
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import numpy as np
from typing_extensions import final

"""Import data from Kaggle"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("mhm707/fd-nfd")

print("Path to dataset files:", path)

# Dynamically find FD and NFD directories within the downloaded path
# The path structure is: .../versions/1/FD and .../versions/1/NFD
dataset_base_path = path
if not os.path.exists(dataset_base_path):
    # Try to find the dataset in common cache locations
    possible_paths = [
        os.path.join(os.path.expanduser('~'), '.cache', 'kagglehub', 'datasets', 'mhm707', 'fd-nfd', 'versions', '1'),
        os.path.join('/root', '.cache', 'kagglehub', 'datasets', 'mhm707', 'fd-nfd', 'versions', '1'),
    ]
    for possible_path in possible_paths:
        if os.path.exists(possible_path):
            dataset_base_path = possible_path
            break

# Find FD and NFD directories
fd_source_path = None
nfd_source_path = None

# Check if FD and NFD are direct subdirectories
for item in os.listdir(dataset_base_path):
    item_path = os.path.join(dataset_base_path, item)
    if os.path.isdir(item_path):
        if item == 'FD':
            fd_source_path = item_path
        elif item == 'NFD':
            nfd_source_path = item_path

if not fd_source_path:
    raise FileNotFoundError(f"Could not find FD directory in {dataset_base_path}")

print(f"Found FD directory: {fd_source_path}")

# Note: FD and NFD images are in the same directory (fd_source_path)
# They are separated by filename: FD.xxx.jpg for flowers, NFDxxx.jpg for non-flowers
# So we'll use fd_source_path for both
if not nfd_source_path:
    # FD and NFD are in the same directory, separated by filename
    nfd_source_path = fd_source_path
    print(f"Note: FD and NFD images are in the same directory, separated by filename")

"""Display random 10 imges to look *at* the dataset"""

dataset_path = fd_source_path

# List all files in the directory
all_files = os.listdir(dataset_path)

# Filter for image files (assuming common extensions like jpg, png, jpeg)
image_files = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Sort the image files to get a consistent 'first 10'
image_files.sort()

# Select the first 10 images
first_10_images = image_files[:10]


# Display each image
for img_name in first_10_images:
    img_path = os.path.join(dataset_path, img_name)

"""Cateogrize the data in not flower and flowers"""

# Define the paths to the categories
base_output_dir = "flower_dataset"
fd_category_dir = os.path.join(base_output_dir, 'FD')
nfd_category_dir = os.path.join(base_output_dir, 'NFD')

# Create output directories if they don't exist
os.makedirs(fd_category_dir, exist_ok=True)
os.makedirs(nfd_category_dir, exist_ok=True)

# Copy images from source directories to local directories
# Note: FD and NFD images are in the same directory, separated by filename
print("Separating and copying FD and NFD images...")
all_source_files = [f for f in os.listdir(fd_source_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Separate FD and NFD images based on filename patterns
# FD images start with "FD." and NFD images start with "NFD"
fd_source_files = [f for f in all_source_files if f.startswith('FD.')]
nfd_source_files = [f for f in all_source_files if f.startswith('NFD')]

print(f"Found {len(fd_source_files)} FD images and {len(nfd_source_files)} NFD images")

# Copy FD images
for filename in fd_source_files:
    src_path = os.path.join(fd_source_path, filename)
    dst_path = os.path.join(fd_category_dir, filename)
    if not os.path.exists(dst_path):  # Only copy if not already exists
        shutil.copy(src_path, dst_path)
print(f"Copied {len(fd_source_files)} FD images")

# Copy NFD images
for filename in nfd_source_files:
    src_path = os.path.join(fd_source_path, filename)
    dst_path = os.path.join(nfd_category_dir, filename)
    if not os.path.exists(dst_path):  # Only copy if not already exists
        shutil.copy(src_path, dst_path)
print(f"Copied {len(nfd_source_files)} NFD images")

# Get the list of files in each category
fd_files = [f for f in os.listdir(fd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
nfd_files = [f for f in os.listdir(nfd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Get the counts
count_fd = len(fd_files)
count_nfd = len(nfd_files)

# Determine which category has more files and needs to be undersampled
if count_fd > count_nfd:
    target_count = count_nfd


    # Calculate how many images to remove from FD
    num_to_remove = count_fd - target_count

    # Randomly select files to remove
    files_to_remove = random.sample(fd_files, num_to_remove)

    # Remove the selected files
    for filename in files_to_remove:
        os.remove(os.path.join(fd_category_dir, filename))


elif count_nfd > count_fd:
    target_count = count_fd



    num_to_remove = count_nfd - target_count


    files_to_remove = random.sample(nfd_files, num_to_remove)

    # Remove the selected files
    for filename in files_to_remove:
        os.remove(os.path.join(nfd_category_dir, filename))


# Verify counts after operation
count_fd_after = len([f for f in os.listdir(fd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
count_nfd_after = len([f for f in os.listdir(nfd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])



"""Resize the images to a 100 by 100"""

# Define base directories
base_output_dir = "flower_dataset"
base_resized_output_dir = "flower_dataset_resized"

# Define source category directories
fd_category_dir = os.path.join(base_output_dir, 'FD')
nfd_category_dir = os.path.join(base_output_dir, 'NFD')

# Define target resized category directories
fd_resized_dir = os.path.join(base_resized_output_dir, 'FD')
nfd_resized_dir = os.path.join(base_resized_output_dir, 'NFD')

# Create the resized directories if they don't exist
os.makedirs(fd_resized_dir, exist_ok=True)
os.makedirs(nfd_resized_dir, exist_ok=True)

image_size = (100, 100) # Changed image size to 100x100

def resize_and_save_images(source_dir, destination_dir, target_size):
    print(f"Resizing and saving images from '{source_dir}' to '{destination_dir}'...")
    for filename in os.listdir(source_dir):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_path = os.path.join(source_dir, filename)
            try:
                img = Image.open(img_path)
                img = img.resize(target_size)
                # Save with original filename to destination
                img.save(os.path.join(destination_dir, filename))
            except Exception as e:
                pass

# Process FD images
resize_and_save_images(fd_category_dir, fd_resized_dir, image_size)

# Process NFD images
resize_and_save_images(nfd_category_dir, nfd_resized_dir, image_size)

"""Split resized images into train and validation sets"""

# Define base directories
base_resized_output_dir = "flower_dataset_resized"

# Define source category directories within the resized dataset
fd_resized_dir = os.path.join(base_resized_output_dir, 'FD')
nfd_resized_dir = os.path.join(base_resized_output_dir, 'NFD')

# Define target split directories
base_train_dir = os.path.join(base_resized_output_dir, 'train')
base_val_dir = os.path.join(base_resized_output_dir, 'validation')

# Create train and validation directories for each category
os.makedirs(os.path.join(base_train_dir, 'FD'), exist_ok=True)
os.makedirs(os.path.join(base_train_dir, 'NFD'), exist_ok=True)
os.makedirs(os.path.join(base_val_dir, 'FD'), exist_ok=True)
os.makedirs(os.path.join(base_val_dir, 'NFD'), exist_ok=True)

def split_and_copy_images(source_dir, train_dest_dir, val_dest_dir, split_ratio=0.7):
    all_files = [f for f in os.listdir(source_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    if not all_files:
        print(f"Warning: No images found in {source_dir}")
        return

    print(f"Splitting {len(all_files)} images from {source_dir}...")
    train_files, val_files = train_test_split(all_files, test_size=1-split_ratio, random_state=42)

    # Copy files to train directory
    for filename in train_files:
        src_path = os.path.join(source_dir, filename)
        dst_path = os.path.join(train_dest_dir, filename)
        shutil.copy(src_path, dst_path)

    # Copy files to validation directory
    for filename in val_files:
        src_path = os.path.join(source_dir, filename)
        dst_path = os.path.join(val_dest_dir, filename)
        shutil.copy(src_path, dst_path)
    
    print(f"Copied {len(train_files)} to train, {len(val_files)} to validation")

# Split and copy images for FD category
split_and_copy_images(fd_resized_dir,
                      os.path.join(base_train_dir, 'FD'),
                      os.path.join(base_val_dir, 'FD'))

# Split and copy images for NFD category
split_and_copy_images(nfd_resized_dir,
                      os.path.join(base_train_dir, 'NFD'),
                      os.path.join(base_val_dir, 'NFD'))


def display_random_images(directory, num_images=5, title_prefix=""):
    all_images = [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    if len(all_images) == 0:
        print(f"No images found in {directory} to display.")
        return

    selected_images = random.sample(all_images, min(num_images, len(all_images)))

    for img_name in selected_images:
        img_path = os.path.join(directory, img_name)
        img = Image.open(img_path)
        plt.imshow(img)
        plt.title(f"{title_prefix}{img_name} (Resized)")
        plt.axis('off')
        plt.show()

img_height, img_width = 100, 100
batch_size = 32


train_dir = 'flower_dataset_resized/train'
validation_dir = 'flower_dataset_resized/validation'

# Data augmentation and rescaling for training data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)


validation_datagen = ImageDataGenerator(
    rescale=1./255
)


train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'
)

"""Creating the nn strcuture"""

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=10,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size
)

# Save the trained model
model_path = os.path.join(os.path.dirname(__file__), 'flower_classifier_model.h5')
model.save(model_path)
print(f"Model saved to: {model_path}")


my_image_path = '/content/photo-1695897706183-5295269554b9.jpg'

final_answer = 0;

if not os.path.exists(my_image_path):
    print(f"Error: Image not found at {my_image_path}. Please check the path and make sure the image is uploaded.")
else:

    preprocessed_new_image = preprocess_image(my_image_path)


    new_raw_prediction = model.predict(preprocessed_new_image)


    new_prediction_score = new_raw_prediction[0][0]
    if new_prediction_score < 0.3:
        new_predicted_class = 'Flower'
    else:
        new_predicted_class = 'Not a Flower'

    print(f"Prediction: {new_predicted_class}")

    if new_predicted_class == 'Flower':
      final_answer = 1
    else:
      final_answer = 0
print(final_answer)