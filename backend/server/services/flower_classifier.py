# -*- coding: utf-8 -*-
"""flower_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xn1oUsyjeOAa2rBNGhWyTN8hIUZBtJP1
"""

import os
from PIL import Image
import matplotlib.pyplot as plt
import random
import shutil
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import numpy as np
from typing_extensions import final

"""Import data from Kaggle"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("mhm707/fd-nfd")

print("Path to dataset files:", path)

"""Display random 10 imges to look *at* the dataset"""

dataset_path = '/root/.cache/kagglehub/datasets/mhm707/fd-nfd/versions/1/FD'

# List all files in the directory
all_files = os.listdir(dataset_path)

# Filter for image files (assuming common extensions like jpg, png, jpeg)
image_files = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Sort the image files to get a consistent 'first 10'
image_files.sort()

# Select the first 10 images
first_10_images = image_files[:10]


# Display each image
for img_name in first_10_images:
    img_path = os.path.join(dataset_path, img_name)

"""Cateogrize the data in not flower and flowers"""

# Define the paths to the categories
base_output_dir = "flower_dataset"
fd_category_dir = os.path.join(base_output_dir, 'FD')
nfd_category_dir = os.path.join(base_output_dir, 'NFD')

# Get the list of files in each category
fd_files = [f for f in os.listdir(fd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
nfd_files = [f for f in os.listdir(nfd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Get the counts
count_fd = len(fd_files)
count_nfd = len(nfd_files)

# Determine which category has more files and needs to be undersampled
if count_fd > count_nfd:
    target_count = count_nfd


    # Calculate how many images to remove from FD
    num_to_remove = count_fd - target_count

    # Randomly select files to remove
    files_to_remove = random.sample(fd_files, num_to_remove)

    # Remove the selected files
    for filename in files_to_remove:
        os.remove(os.path.join(fd_category_dir, filename))


elif count_nfd > count_fd:
    target_count = count_fd



    num_to_remove = count_nfd - target_count


    files_to_remove = random.sample(nfd_files, num_to_remove)

    # Remove the selected files
    for filename in files_to_remove:
        os.remove(os.path.join(nfd_category_dir, filename))


# Verify counts after operation
count_fd_after = len([f for f in os.listdir(fd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
count_nfd_after = len([f for f in os.listdir(nfd_category_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])



"""Make sure the data has the same amount of flowers and not flower data points"""

# Define base directories
base_resized_output_dir = "flower_dataset_resized"

# Define source category directories within the resized dataset
fd_resized_dir = os.path.join(base_resized_output_dir, 'FD')
nfd_resized_dir = os.path.join(base_resized_output_dir, 'NFD')

# Define target split directories
base_train_dir = os.path.join(base_resized_output_dir, 'train')
base_val_dir = os.path.join(base_resized_output_dir, 'validation')

# Create train and validation directories for each category
os.makedirs(os.path.join(base_train_dir, 'FD'), exist_ok=True)
os.makedirs(os.path.join(base_train_dir, 'NFD'), exist_ok=True)
os.makedirs(os.path.join(base_val_dir, 'FD'), exist_ok=True)
os.makedirs(os.path.join(base_val_dir, 'NFD'), exist_ok=True)

def split_and_copy_images(source_dir, train_dest_dir, val_dest_dir, split_ratio=0.7):
    all_files = [f for f in os.listdir(source_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    if not all_files:

        return

    train_files, val_files = train_test_split(all_files, test_size=1-split_ratio, random_state=42)


    # Copy files to train directory
    for filename in train_files:
        src_path = os.path.join(source_dir, filename)
        dst_path = os.path.join(train_dest_dir, filename)
        shutil.copy(src_path, dst_path)

    # Copy files to validation directory
    for filename in val_files:
        src_path = os.path.join(source_dir, filename)
        dst_path = os.path.join(val_dest_dir, filename)
        shutil.copy(src_path, dst_path)


# Split and copy images for FD category
split_and_copy_images(fd_resized_dir,
                      os.path.join(base_train_dir, 'FD'),
                      os.path.join(base_val_dir, 'FD'))

# Split and copy images for NFD category
split_and_copy_images(nfd_resized_dir,
                      os.path.join(base_train_dir, 'NFD'),
                      os.path.join(base_val_dir, 'NFD'))

"""Resize the images to a 100 by 100"""

# Define base directories
base_output_dir = "flower_dataset"
base_resized_output_dir = "flower_dataset_resized"

# Define source category directories
fd_category_dir = os.path.join(base_output_dir, 'FD')
nfd_category_dir = os.path.join(base_output_dir, 'NFD')

# Define target resized category directories
fd_resized_dir = os.path.join(base_resized_output_dir, 'FD')
nfd_resized_dir = os.path.join(base_resized_output_dir, 'NFD')

# Create the resized directories if they don't exist
os.makedirs(fd_resized_dir, exist_ok=True)
os.makedirs(nfd_resized_dir, exist_ok=True)

image_size = (100, 100) # Changed image size to 100x100

def resize_and_save_images(source_dir, destination_dir, target_size):
    print(f"Resizing and saving images from '{source_dir}' to '{destination_dir}'...")
    for filename in os.listdir(source_dir):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            img_path = os.path.join(source_dir, filename)
            try:
                img = Image.open(img_path)
                img = img.resize(target_size)
                # Save with original filename to destination
                img.save(os.path.join(destination_dir, filename))
            except Exception as e:
                pass

# Process FD images
resize_and_save_images(fd_category_dir, fd_resized_dir, image_size)

# Process NFD images
resize_and_save_images(nfd_category_dir, nfd_resized_dir, image_size)


def display_random_images(directory, num_images=5, title_prefix=""):
    all_images = [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    if len(all_images) == 0:
        print(f"No images found in {directory} to display.")
        return

    selected_images = random.sample(all_images, min(num_images, len(all_images)))

    for img_name in selected_images:
        img_path = os.path.join(directory, img_name)
        img = Image.open(img_path)
        plt.imshow(img)
        plt.title(f"{title_prefix}{img_name} (Resized)")
        plt.axis('off')
        plt.show()

img_height, img_width = 100, 100
batch_size = 32


train_dir = 'flower_dataset_resized/train'
validation_dir = 'flower_dataset_resized/validation'

# Data augmentation and rescaling for training data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)


validation_datagen = ImageDataGenerator(
    rescale=1./255
)


train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'
)

"""Creating the nn strcuture"""

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=10,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size
)

def preprocess_image(image_path):
    """Preprocess an image for prediction"""
    img = Image.open(image_path)
    img = img.resize((img_height, img_width))
    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    return img_array

my_image_path = '/content/photo-1695897706183-5295269554b9.jpg'

final_answer = 0;

if not os.path.exists(my_image_path):
    print(f"Error: Image not found at {my_image_path}. Please check the path and make sure the image is uploaded.")
else:

    preprocessed_new_image = preprocess_image(my_image_path)


    new_raw_prediction = model.predict(preprocessed_new_image)


    new_prediction_score = new_raw_prediction[0][0]
    if new_prediction_score < 0.5:
        new_predicted_class = 'Flower'
    else:
        new_predicted_class = 'Not a Flower'

    print(f"Prediction: {new_predicted_class}")

    if new_predicted_class == 'Flower':
      final_answer = 1
    else:
      final_answer = 0
print(final_answer)